{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Similarity Detection with FaceNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    \"\"\"\n",
    "    Load an image and convert it in 'RGB'\n",
    "    \n",
    "    Return an image\n",
    "    \"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "\n",
    "    \n",
    "def plot_img_list(img_list):\n",
    "    \"\"\"\n",
    "    Display images side by side\n",
    "    \n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "    for i, img in enumerate(img_list):\n",
    "        fig.add_subplot(1, len(img_list), i+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = os.getcwd()\n",
    "print(f\"Your working directory : {working_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add images you want to play with in a folder called *'test_images'* in your working directory for simplicity\n",
    "img_dir = os.path.join(working_dir, 'test_images')\n",
    "crop_dir = os.path.join(working_dir, 'crop_images')\n",
    "if os.path.exists(img_dir):\n",
    "    print(f\"Your 'test_images' directory : {img_dir}\")\n",
    "else:\n",
    "    print(\"Please create a folder called 'test_images' in your working directory\")\n",
    "if os.path.exists(crop_dir):\n",
    "    print(f\"Your 'cropped_dir' directory : {crop_dir}\")\n",
    "else:\n",
    "    print(\"Please create a folder called 'crop_images' in your working directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_filenames = os.listdir(img_dir)\n",
    "print(img_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = [pil_loader(os.path.join(img_dir, path)) for path in tqdm(img_filenames)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img_list(img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face detection and cropping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect and crop faces with MTCNN\n",
    "\n",
    "**Multi-task convolutional neural network (MTCNN)**, works in three steps and use one neural network for each. The first part is a proposal network. It will predict potential face positions and their bounding boxes like an attention network in Faster R-CNN. The result of this step is a large number of face detections and lots of false detections. The second part uses images and outputs of the first prediction. It makes a refinement of the result to eliminate most of false detections and aggregate bounding boxes. The last part refines even more the predictions and adds facial landmarks predictions.\n",
    "\n",
    "<img src=\"img_pres/MTCNN.png\" width=\"350\"> \n",
    "Image from : https://www.mdpi.com/2076-3417/9/18/3774/htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the device : GPU / CPU on which torch runs\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Running on device: {device}\")\n",
    "\n",
    "# Load the mtcnn model : the model that allows face detection and cropping\n",
    "mtcnn = MTCNN(image_size=160, margin=0, min_face_size=20, thresholds=[0.6, 0.7, 0.7], \n",
    "              factor=0.709, post_process=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that loops over a list of images, detect & crops faces or add them to invalid images list (no face detected)\n",
    "def detect_faces(img_list, img_filenames, mtcnn):\n",
    "    \"\"\"\n",
    "    Detect face on an image and save the croppped face\n",
    "    \n",
    "    Return a tensor list of aligned images and an image list of invalid images\n",
    "    \"\"\"\n",
    "    aligned_tensor = []\n",
    "    invalid_imgs = []\n",
    "\n",
    "    for i, img in enumerate(tqdm(img_list)):\n",
    "        img_aligned = mtcnn(img, return_prob=False, save_path=os.path.join(crop_dir, img_filenames[i]))\n",
    "        \n",
    "        if img_aligned is not None:\n",
    "            aligned_tensor.append(img_aligned)\n",
    "        else:\n",
    "            invalid_imgs.append(img)\n",
    "\n",
    "    aligned_tensor = torch.stack(aligned_tensor).to(device)\n",
    "    \n",
    "    return aligned_tensor, invalid_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_tensor, invalid_imgs = detect_faces(img_list, img_filenames, mtcnn)\n",
    "\n",
    "if len(invalid_imgs):\n",
    "    print(\"Invalid images detected\")\n",
    "    print(\"Please romove invalid images detected\")\n",
    "    plot_img_list(invalid_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_imgs = [pil_loader(os.path.join(img_dir, os.path.join(crop_dir, path))) for path in tqdm(img_filenames)]\n",
    "plot_img_list(crop_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Create embeddings from pretrained model\n",
    "\n",
    "**FaceNet** provides a unified embedding for face recognition, verification and clustering tasks. It maps each face image into a euclidean space such that the distances in that space correspond to face similarity, i.e. an image of person A will be placed closer to all the other images of person A as compared to images of any other person present in the dataset.\n",
    "\n",
    "The main difference between FaceNet and other techniques is that it learns the mapping from the images and creates embeddings rather than using any bottleneck layer for recognition or verification tasks. \n",
    "\n",
    "<img src=\"img_pres/facenet.png\" width=\"500\">\n",
    "\n",
    "**FaceNet** uses deep convolutional neural network (CNN). The network is trained such that the squared L2 distance between the embeddings correspond to face similarity. Thanks to the triplet loss function the model can learn that we want our anchor image (image of a specific person A) to be closer to positive images (all the images of person A) as compared to negative images (all the other images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Inception Resnet model : the model that will calculate embeddings based on cropped faces\n",
    "pretrained_model = 'vggface2'\n",
    "resnet = InceptionResnetV1(pretrained=pretrained_model).eval().to(device)\n",
    "embeddings = resnet(aligned_tensor).detach().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Calculate distances between embeddings (images)\n",
    "\n",
    "<img src=\"img_pres/cosin_vs_l2.png\" width=\"500\">\n",
    "\n",
    "This is a visual representation of **euclidean distance (d)** and **cosine similarity (θ)**. While cosine looks at the angle between vectors (thus not taking into regard their weight or magnitude), euclidean distance is similar to using a ruler to actually measure the distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_l2(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Calculate the Euclidean norm (L2) of 2 tensors\n",
    "    \n",
    "    Return a disctionary with a distance type and value\n",
    "    Range [0:∞] where 0 means shortest distance (perfect similarity) and ∞ means longest distance (lowest similarity)\n",
    "    \"\"\"\n",
    "    distance = (embedding1 - embedding2).norm()\n",
    "    \n",
    "    distance_dict = {\n",
    "        \"distance_type\" : \"L2\",\n",
    "        \"distance_value\" : distance\n",
    "    }\n",
    "    \n",
    "    return distance_dict\n",
    "\n",
    "\n",
    "def calculate_distance_cosin(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Calculate the cosin similarity (angle) of 2 tensors\n",
    "    \n",
    "    Return a disctionary with a distance type and value\n",
    "    (Range [-1:1] where -1 means opposite similarity and 1 means perfect similarity)\n",
    "    \"\"\"\n",
    "    distance = torch.nn.functional.cosine_similarity(embedding1, embedding2, dim=0)\n",
    "    \n",
    "    distance_dict = {\n",
    "        \"distance_type\" : \"Cosin\",\n",
    "        \"distance_value\" : distance\n",
    "    }\n",
    "    \n",
    "    return distance_dict\n",
    "\n",
    "\n",
    "def similarity_detection(distance_dict, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Apply a threshold to a distance\n",
    "    \"\"\"\n",
    "    match_emoji = \"\\U0001f600\"\n",
    "    no_match_emoji = \"\\U0001F62D\"\n",
    "    \n",
    "    distance_type = distance_dict.get(\"distance_type\")\n",
    "    distance_value = distance_dict.get(\"distance_value\")\n",
    "    \n",
    "    print(f\"Distance type : {distance_type}\")\n",
    "    \n",
    "    if distance_type == \"L2\" and distance_value <= threshold:\n",
    "        print(f\"Same identity {match_emoji}\")\n",
    "        \n",
    "    elif distance_type == \"Cosin\" and distance_value >= threshold:\n",
    "        print(f\"Same identity {match_emoji}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Different identities {no_match_emoji}\")\n",
    "    \n",
    "    print(f\"With a threshold of : {threshold:.2f}\")\n",
    "    print(f\"With a distance of : {distance_value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index position\n",
    "i_img1, i_img2 = 0, 1\n",
    "img_to_compare = [img_list[i_img1], img_list[i_img2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img_list(img_to_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_dict_l2 = calculate_distance_l2(embeddings[i_img1], embeddings[i_img2])\n",
    "distance_dict_cosin = calculate_distance_cosin(embeddings[i_img1], embeddings[i_img2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_value_l2 = distance_dict_l2.get(\"distance_value\")\n",
    "distance_value_cosin = distance_dict_cosin.get(\"distance_value\")\n",
    "\n",
    "print(f\"Distance : Euclidean norm = {distance_value_l2:.2f}\")\n",
    "print(f\"Distance : Cosin = {distance_value_cosin:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_detection(distance_dict_l2, threshold=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index position\n",
    "i_img1, i_img2 = 0, 3\n",
    "img_to_compare = [img_list[i_img1], img_list[i_img2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img_list(img_to_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_dict_l2 = calculate_distance_l2(embeddings[i_img1], embeddings[i_img2])\n",
    "distance_dict_cosin = calculate_distance_cosin(embeddings[i_img1], embeddings[i_img2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_value_l2 = distance_dict_l2.get(\"distance_value\")\n",
    "distance_value_cosin = distance_dict_cosin.get(\"distance_value\")\n",
    "\n",
    "print(f\"Distance : Euclidean norm = {distance_value_l2:.2f}\")\n",
    "print(f\"Distance : Cosin = {distance_value_cosin:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_detection(distance_dict_l2, threshold=0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
